# Getting started


This page describes the typical ModelSkill workflow for comparing model
results and observations. 

## Installation

```bash
pip install modelskill
```

## Skill assessment

The simplest use-case for skill assessment is when you have a dataset of matched model results and observations in tabular format.

```{python}
import pandas as pd
import modelskill as ms
df = pd.read_csv("../data/Vistula/sim1/6158100.csv", parse_dates=True, index_col="Date")
df.head()
```

```{python}
cmp = ms.from_matched(df, obs_item="Qobs", mod_items="Qsim", quantity=ms.Quantity("Discharge", "m3/s"))
cmp
```

A time series plot is a common way to visualize the comparison.

```{python}
cmp.plot.timeseries()
```

Another more quantitative way to compare the model and observations is to use a scatter plot, which optionally includes a skill table ([Definition of the metrics](`modelskill.metrics`)).

```{python}
cmp.plot.scatter(skill_table=True)
```

The skill table can also be produced in tabular format, including specifing other metrics.

```{python}
cmp.skill(metrics=["bias", "mae", "rmse", "kge", "si"])
```