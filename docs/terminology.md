# Terminology

`modelskill` is a library for assessing the skill of numerical models. It provides tools for comparing model results with observations and calculating validation metrics. This page defines some of the key terms used in the documentation.


## General terminology 

### Skill
[**Skill**](modelskill.comparison.ComparerCollection.skill) refers to the ability of a numerical model to accurately represent the real-world phenomenon it aims to simulate. It is a measure of how well the model performs in reproducing the observed system. Skill can be assessed using various metrics, such as accuracy, precision, and reliability, depending on the specific goals of the model and the nature of the data.


### Validation
**Validation** is the process of assessing the model's performance by comparing its output to real-world observations or data collected from the system being modeled. It helps ensure that the model accurately represents the system it simulates. Validation is typically performed before the model is used for prediction or decision-making.


### Calibration
**Calibration** is the process of adjusting the model's parameters or settings to improve its performance. It involves fine-tuning the model to better match observed data. Calibration aims to reduce discrepancies between model predictions and actual measurements. At the end of the calibration process, the calibrated model should be validated with independent data.


### Performance
**Performance** is a measure of how well a numerical model operates in reproducing the observed system. It can be assessed using various metrics, such as accuracy, precision, and reliability, depending on the specific goals of the model and the nature of the data. In this context, **performance** is synonymous with **skill**.


### Timeseries
A **timeseries** is a sequence of data points in time. The data can either be from [observations](#observation) or [model results](#model-result). Timeseries can univariate or multivariate; ModelSkill primarily supports univariate timeseries. Multivariate timeseries can be assessed one variable at a time. Timeseries can also have different spatial dimensions, such as point, track, line, or area.


### Observation
An [**observation**](modelskill.observation.PointObservation) refers to real-world data or measurements collected from the system you are modeling. Observations serve as a reference for assessing the model's performance. These data points are used to compare with the model's predictions during validation and calibration. Observations are usually based on field measurements or laboratory experiments, but for the purposes of model validation, they can also be derived from other models.


### Measurement
A **measurement** is a single data point or value collected from the system being modeled. It is a single observation. Measurements are used to compare with the model's predictions during validation and calibration. Measurements are usually based on field measurements or laboratory experiments, but for the purposes of model validation, they can also be derived from other models.


### Model result
A [**model result**](modelskill.model.PointModelResult) is the output of any type of numerical model. It is the data generated by the model during a simulation. Model results can be compared with observations to assess the model's performance. In the context of validation, the term "model result" is often used interchangeably with "model output" or "model prediction."


### Metric
A **metric** is a quantitative measure (a mathematical expression) used to evaluate the performance of a numerical model. Metrics provide a standardized way to assess the model's accuracy, precision, and other attributes. A metric aggregates the skill of a model into a single number. See list of [metrics](modelskill.metrics) supported by `ModelSkill`.


### Score
A **score** is a numerical value that summarizes the model's performance based on chosen metrics. Scores can be used to rank or compare different models or model configurations. In the context of validation, the "skill score" or "validation score" often quantifies the model's overall performance. The score of a model is a single number, calculated as a weighted average for all time-steps, observations and variables. If you want to perform automated calibration, you can use the score as the objective function.


## ModelSkill-specific terminology

### matched data
In ModelSkill, observations and model results are *matched* when they refer to the same positions in space and time. If the observations and model results are already matched, the [modelskill.from_matched](modelskill.from_matched) function can be used to create a [Comparer](#comparer) directly. Otherwise, the [compare](#compare) function can be used to match the observations and model results in space and time. 


### compare
The function [compare](modelskill.comparison.compare) is used to compare a model result with observations. It returns a [Comparer](modelskill.comparison.Comparer) object.


### Comparer
A [**Comparer**](modelskill.comparison.Comparer) is an object that compares a model result with observations. It is used to calculate validation metrics and generate plots. A Comparer can be created using the [compare](modelskill.comparison.compare) function. See [Comparers](modelskill.comparison) for more details.


### ComparerCollection
A [**ComparerCollection**](modelskill.comparison.ComparerCollection) is a collection of Comparers. It is used to compare multiple model results with multiple observations. A ComparerCollection can be created using the [compare_collection](modelskill.comparison.compare_collection) function. See [ComparerCollections](modelskill.comparison) for more details.


### Connector
In past versions of FMSkill/ModelSkill, the Connector class was used to connect observations and model results. This class has been deprecated and is no longer in use. 

