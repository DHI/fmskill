{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metric <function bias at 0x7f86bec2cf70> not defined. Choose from () or use `add_metric` to add a custom metric.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/src/modelskill/notebooks/Untitled.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jan/src/modelskill/notebooks/Untitled.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mmod\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]}, index\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mdate_range(\u001b[39m'\u001b[39m\u001b[39m2018-01-01\u001b[39m\u001b[39m'\u001b[39m, periods\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jan/src/modelskill/notebooks/Untitled.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m cmp \u001b[39m=\u001b[39m ms\u001b[39m.\u001b[39mfrom_matched(df)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jan/src/modelskill/notebooks/Untitled.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m cmp\u001b[39m.\u001b[39;49mplot\u001b[39m.\u001b[39;49mscatter(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmod\u001b[39;49m\u001b[39m\"\u001b[39;49m, skill_table\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_comparer_plotter.py:511\u001b[0m, in \u001b[0;36mComparerPlotter.scatter\u001b[0;34m(self, model, bins, quantiles, fit_to_quantiles, show_points, show_hist, show_density, norm, backend, figsize, xlim, ylim, reg_method, title, xlabel, ylabel, skill_table, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m skill_table:\n\u001b[1;32m    510\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m skill_table \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m skill_table\n\u001b[0;32m--> 511\u001b[0m     skill_df \u001b[39m=\u001b[39m cmp\u001b[39m.\u001b[39;49mskill(metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[1;32m    512\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m         units \u001b[39m=\u001b[39m unit_text\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_comparison.py:957\u001b[0m, in \u001b[0;36mComparer.skill\u001b[0;34m(self, by, metrics, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mskill\u001b[39m(\n\u001b[1;32m    911\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    912\u001b[0m     by: Optional[Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    913\u001b[0m     metrics: Optional[\u001b[39mlist\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    915\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AggregatedSkill:\n\u001b[1;32m    916\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Skill assessment of model(s)\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \n\u001b[1;32m    918\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39m    2017-10-29  41  0.33  0.41   0.25  0.36  0.96  0.06  0.99\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 957\u001b[0m     metrics \u001b[39m=\u001b[39m _parse_metric(metrics, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetrics, return_list\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    959\u001b[0m     \u001b[39m# TODO remove in v1.1\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     model, start, end, area \u001b[39m=\u001b[39m _get_deprecated_args(kwargs)\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_utils.py:20\u001b[0m, in \u001b[0;36m_parse_metric\u001b[0;34m(metric, default_metrics, return_list)\u001b[0m\n\u001b[1;32m     18\u001b[0m     metric \u001b[39m=\u001b[39m mtr\u001b[39m.\u001b[39mget_metric(metric)\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric, Iterable):\n\u001b[0;32m---> 20\u001b[0m     metrics \u001b[39m=\u001b[39m [_parse_metric(m, default_metrics) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m metric]\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics\n\u001b[1;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(metric):\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_utils.py:20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m     metric \u001b[39m=\u001b[39m mtr\u001b[39m.\u001b[39mget_metric(metric)\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric, Iterable):\n\u001b[0;32m---> 20\u001b[0m     metrics \u001b[39m=\u001b[39m [_parse_metric(m, default_metrics) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m metric]\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics\n\u001b[1;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(metric):\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_utils.py:18\u001b[0m, in \u001b[0;36m_parse_metric\u001b[0;34m(metric, default_metrics, return_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m     metric \u001b[39m=\u001b[39m default_metrics\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric, (\u001b[39mstr\u001b[39m, Callable)):\n\u001b[0;32m---> 18\u001b[0m     metric \u001b[39m=\u001b[39m mtr\u001b[39m.\u001b[39;49mget_metric(metric)\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric, Iterable):\n\u001b[1;32m     20\u001b[0m     metrics \u001b[39m=\u001b[39m [_parse_metric(m, default_metrics) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m metric]\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/metrics.py:738\u001b[0m, in \u001b[0;36mget_metric\u001b[0;34m(metric)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[39mreturn\u001b[39;00m metric\n\u001b[1;32m    737\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    739\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMetric \u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m}\u001b[39;00m\u001b[39m not defined. Choose from \u001b[39m\u001b[39m{\u001b[39;00mdefined_metrics\u001b[39m}\u001b[39;00m\u001b[39m or use `add_metric` to add a custom metric.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Metric <function bias at 0x7f86bec2cf70> not defined. Choose from () or use `add_metric` to add a custom metric."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import modelskill as ms\n",
    "\n",
    "df = pd.DataFrame({'obs': [1, 2, 3], 'mod': [1, 2, 3]}, index=pd.date_range('2018-01-01', periods=3, freq='D'))\n",
    "cmp = ms.from_matched(df)\n",
    "cmp.plot.scatter(model=\"mod\", skill_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metric <function bias at 0x7f86bec2cf70> not defined. Choose from () or use `add_metric` to add a custom metric.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/src/modelskill/notebooks/Untitled.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jan/src/modelskill/notebooks/Untitled.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cmp\u001b[39m.\u001b[39;49mplot\u001b[39m.\u001b[39;49mscatter(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmod\u001b[39;49m\u001b[39m\"\u001b[39;49m, skill_table\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_comparer_plotter.py:511\u001b[0m, in \u001b[0;36mComparerPlotter.scatter\u001b[0;34m(self, model, bins, quantiles, fit_to_quantiles, show_points, show_hist, show_density, norm, backend, figsize, xlim, ylim, reg_method, title, xlabel, ylabel, skill_table, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m skill_table:\n\u001b[1;32m    510\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m skill_table \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m skill_table\n\u001b[0;32m--> 511\u001b[0m     skill_df \u001b[39m=\u001b[39m cmp\u001b[39m.\u001b[39;49mskill(metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[1;32m    512\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m         units \u001b[39m=\u001b[39m unit_text\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_comparison.py:957\u001b[0m, in \u001b[0;36mComparer.skill\u001b[0;34m(self, by, metrics, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mskill\u001b[39m(\n\u001b[1;32m    911\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    912\u001b[0m     by: Optional[Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    913\u001b[0m     metrics: Optional[\u001b[39mlist\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    915\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AggregatedSkill:\n\u001b[1;32m    916\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Skill assessment of model(s)\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \n\u001b[1;32m    918\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39m    2017-10-29  41  0.33  0.41   0.25  0.36  0.96  0.06  0.99\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 957\u001b[0m     metrics \u001b[39m=\u001b[39m _parse_metric(metrics, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetrics, return_list\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    959\u001b[0m     \u001b[39m# TODO remove in v1.1\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     model, start, end, area \u001b[39m=\u001b[39m _get_deprecated_args(kwargs)\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_utils.py:20\u001b[0m, in \u001b[0;36m_parse_metric\u001b[0;34m(metric, default_metrics, return_list)\u001b[0m\n\u001b[1;32m     18\u001b[0m     metric \u001b[39m=\u001b[39m mtr\u001b[39m.\u001b[39mget_metric(metric)\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric, Iterable):\n\u001b[0;32m---> 20\u001b[0m     metrics \u001b[39m=\u001b[39m [_parse_metric(m, default_metrics) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m metric]\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics\n\u001b[1;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(metric):\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_utils.py:20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m     metric \u001b[39m=\u001b[39m mtr\u001b[39m.\u001b[39mget_metric(metric)\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric, Iterable):\n\u001b[0;32m---> 20\u001b[0m     metrics \u001b[39m=\u001b[39m [_parse_metric(m, default_metrics) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m metric]\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics\n\u001b[1;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(metric):\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/comparison/_utils.py:18\u001b[0m, in \u001b[0;36m_parse_metric\u001b[0;34m(metric, default_metrics, return_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m     metric \u001b[39m=\u001b[39m default_metrics\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric, (\u001b[39mstr\u001b[39m, Callable)):\n\u001b[0;32m---> 18\u001b[0m     metric \u001b[39m=\u001b[39m mtr\u001b[39m.\u001b[39;49mget_metric(metric)\n\u001b[1;32m     19\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric, Iterable):\n\u001b[1;32m     20\u001b[0m     metrics \u001b[39m=\u001b[39m [_parse_metric(m, default_metrics) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m metric]\n",
      "File \u001b[0;32m~/src/modelskill/modelskill/metrics.py:738\u001b[0m, in \u001b[0;36mget_metric\u001b[0;34m(metric)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[39mreturn\u001b[39;00m metric\n\u001b[1;32m    737\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    739\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMetric \u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m}\u001b[39;00m\u001b[39m not defined. Choose from \u001b[39m\u001b[39m{\u001b[39;00mdefined_metrics\u001b[39m}\u001b[39;00m\u001b[39m or use `add_metric` to add a custom metric.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Metric <function bias at 0x7f86bec2cf70> not defined. Choose from () or use `add_metric` to add a custom metric."
     ]
    }
   ],
   "source": [
    "cmp.plot.scatter(model=\"mod\", skill_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
